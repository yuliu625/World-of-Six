# World of Six: Exploring Machine Optimism in Multi-Agent Systems
本项目为论文 "Beyond Rationality: Network Effects, History, and Machine Optimism in Multi-Agents" (SWAIB 2025) 的官方代码仓库。本研究深入探索了基于大型语言模型（LLM）的智能体，在具有网络效应的复杂多智能体博弈环境中的行为、策略及其偏见。

## 摘要
本研究旨在探究在复杂动态博弈环境中，基于 LLM 的智能体的决策行为。我们构建了一个模拟实验，将智能体置于一个具有网络效应的动态博弈场景中，并系统地分析其决策过程。我们的核心发现揭示了这些智能体并非完全理性，而是表现出可观测的系统性偏差。具体而言，我们观察到以下四种关键行为模式:
- 非理性预期与偏差: LLM 智能体的决策并非完全理性，而是存在可观测的系统性偏差。
- 均值倾向: 在缺乏历史信息的情况下，智能体倾向于选择平均值，这表明其决策过程可能受到先验偏见或默认行为的影响。
- 历史趋势: 智能体能够有效地利用历史信息来动态调整决策，展示出一定程度的学习和适应能力。
- 乐观倾向: 智能体倾向于表现出乐观主义，这在某些情况下可能导致其效用受损。我们推测这种偏见可能与 LLM 的 RLHF 训练过程有关。

本研究通过模拟复杂博弈，为理解和评估 LLM 智能体在类现实世界环境中的行为提供了新的视角，并为未来在更复杂、非零和博弈中构建更可靠、可预测的智能体奠定了基础。


## 项目总览
### 研究动机与挑战
现有的博弈论研究，无论是在经济学还是人工智能领域，都面临局限性。经济学研究常依赖于高度简化的理论模型或成本高昂的人类实验；而 AI 领域则多集中于简单的零和博弈。这两种方法均未能充分探究网络效应这种多样化、动态、递归且非线性的复杂互动。

网络效应是一种典型的现实世界现象，其中商品或服务的价值取决于其用户群体的规模。这种相互依赖性使得博弈策略、定价和均衡分析变得异常复杂。本研究旨在填补这一空白，通过构建一个模拟环境来深入分析 LLM 智能体在网络效应下的决策行为。
### 研究核心
我们设计了一项模拟实验，将 LLM 智能体置于一个动态博弈场景中，并分析其决策过程。我们的研究旨在回答以下两个核心问题：
- LLM 智能体在复杂的、类现实世界的动态博弈中，其行为表现是否与人类相似？它们的决策是否更优？
- 在多智能体互动中，机器的行为机制和非理性偏见是如何形成的？

### 实验设计与方法论
我们假设一个6人世界，每个人需要独立决策是否参加一场会议。每个智能体的效用函数$U_j(\theta_j)$定义如下:

$$
U_j(\theta_j) = \theta_j + \beta \times N - p_j
$$

其中:
- $U_j$是智能体$j$的效用。
- $\theta_j$是每个智能体$j$的独立效用。
- $\beta$是网络效用强度。
- $N$网络中的实际参与人数。
- $p_j$是网络商品价格。

通过设定不同的网络效应强度($\beta$)、独立价值($\theta_j$)和价格序列($p_j$)，我们能够系统地分析和记录每个智能体的决策行为，从而揭示其潜在的行为模式。


## 项目结构与技术栈
```bash
.
├── game/                    # 核心博弈逻辑、智能体及运行时环境
│   ├── agents/              # 智能体定义 (participant, manager)
│   ├── configs/             # 智能体属性和价格序列配置
│   ├── prompts/             # 智能体Prompt模板
│   ├── protocols/           # 智能体间通信协议 (schema)
│   ├── runtime/             # 多智能体系统运行逻辑
│   ├── tools/               # 为Manager定义的工具 (e.g., utility_calculator)
│   └── utils/               # 辅助工具
├── experiments/             # 运行论文实验的脚本及配置
├── visualization/           # 数据可视化分析脚本
└── README.md                # 本文件
```

### 关键模块详解
- `game/`: 包含所有与博弈模拟相关的核心代码。`agents/` 文件夹中的 `participant` 被设计为一个通用博弈智能体，易于迁移到其他博弈场景；`manager` 则是专门为本实验设计的、由程序规则控制的代理，负责管理游戏流程和规则。
- `experiments/`: 该目录下的脚本和配置用于运行论文中描述的特定实验。每个实验脚本都经过精心设计，以确保结果的可复现性。
- `visualization/`: 包含用于生成论文中所有图表的代码，包括行为模式分析图、效用曲线图等。这部分代码用于帮助用户理解实验结果。

## 复现实验
### 1. 模型配置: 
在 model_client.py 中注册你的 LLM 模型，例如 OpenAI 或其他服务商。
### 2.参数设置: 
在 run_a_game.py 或 run_an_experiment.py 中，根据你的需要调整参数。你可以参考 experiments 文件夹中的现有配置作为示例。
### 3.数据分析与可视化
实验运行完毕后，使用 visualization/ 目录下的脚本可以生成论文中的图表。


## 维护状态与相关工作
这个仓库是一个早期的研究项目，仅分享论文成果，但后续将不再进行维护。主要原因如下:
- 编程范式演进: 本项目在设计上过度依赖面向对象编程，缺乏更现代化的设计模式。后续工作已转向更具可维护性的函数式编程或更优的设计模式。
- 技术栈迭代: 这只是我对 LLM 智能体研究的初步探索。我已在此基础上进行了更深入的研究，并采用了更成熟的实现方法。
- 后续工作: 本论文的研究思路已通过更先进的方法得到进一步验证和扩展。相关工作正在投稿中，其代码仓库将在论文被接收后公开。
### 相关研究
- [前置研究](https://github.com/yuliu625/Simulate-the-Prisoners-Dilemma-with-Agents): 我基于`autogen`的更早期尝试，研究了 LLM 智能体在囚徒困境等简单博弈场景下的行为。
- [后续研究](): 本论文的研究思路已通过更先进的方法得到进一步验证和扩展。相关工作正在投稿中，其代码仓库将在论文被接收后公开。
### 我的其他仓库
- [Agent-Development-Toolkit](https://github.com/yuliu625/Yu-Agent-Development-Toolkit): 一个基于 `langchain` 和 `langgraph` 的智能体开发工具箱，包含我关于智能体架构和设计的最新实践。
- [Prompt-Collection](https://github.com/yuliu625/Yu-Prompt-Collection): 一个使用 `jinja2` 模板语法管理的 Prompt 库，用于高效管理和复用各种 Prompt 模板。
- [Deep-Learning-Toolkit](https://github.com/yuliu625/Yu-Deep-Learning-Toolkit): 我日常任务中累计的深度学习工具。
- [Flash-Boilerplate](https://github.com/yuliu625/Yu-Flash-Boilerplate): 我为我的深度学习项目构建的基础模板仓库。

